# 极大似然估计（MLE）

了解极大似然估计之前，我们需要了解条件概率，即$P(x|\theta)$ 代表着什么？

对于上述函数： 输入有两个：$x$表示某一个具体的数据，$\theta$表示模型的参数：

如果$\theta$是已知确定的， $x$是变量，这个函数叫做**概率函数**(probability function)，它描述对于不同的样本点$x$出现概率是多少。

如果$x$是已知确定的，$\theta$ 是变量，这个函数叫做**似然函数**(likelihood function), 它描述对于不同的模型参数，出现$\theta$这个样本点的概率是多少。

实际应用如何呢？我们举两个例子

## 抓球

当我们面前有一个箱子，箱子里放了非常多的黑白球，这些黑白球的组成比例我们并不知道，我们现在想要估计这个比例，即求出上述说的这个$\theta$，我们会怎么做呢？

第一种方案是把所有的黑白球都抓出来，我们自然就知道了黑白球的比例，但是这样并不现实，成本也很高，于是我们选择第二种方案。

第二种方案的话，我们把黑白球充分混合后，抓100个球出来，根据这100个球的颜色比例来估计黑白球的比例，而且我们的估计不能乱估计，我们需要找到一种最有可能的估计，使得我们抓出来的100球的颜色比例出现概率是最为可能的。

这时候我们会发现，我们其实是根据结果反推过程，也可以说是根据小样本的结果来估计和推测大样本的构成。

现在我们抓出来了70个白球，30个黑球，我们的直觉会让我们感觉到箱子内黑白球的比例是7：3，我们通过极大似然估计的角度重新看待这个问题。

我们考虑这100次情况同时发生的概率，如下：

$$
\begin{align}P(样本结果|\theta)&=P(x_1,x_2,...,x_{100}|\theta) \\ &=P(x_1|\theta)P(x_2|\theta)...P(x_{100}|\theta)\end{align}
$$

从公式（1）到（2）的关键在于我们认为，每一次的取球之间是独立同分布的，这一假设非常重要，这一假设是极大似然估计的根基。

有人要问了那$P(x_i|\theta)$如何解呢？事实上，我们做了100次实验，假设我们每一次拿出球的概率如下：

|                  | 白球  | 黑球    |
| ---------------- | --- | ----- |
| $P(x_i\|\theta)$ | $p$ | 1-$p$ |

那么实际上，我们的（2）式能写成如下形式：

$$
\begin{align}P(样本结果|\theta)=p^{70}(1-p)^{30}\end{align}
$$

我们会发现该式变成了一个参数为$p$的函数
